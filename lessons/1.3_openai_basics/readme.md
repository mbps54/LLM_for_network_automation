# –û—Å–Ω–æ–≤—ã —Ä–∞–±–æ—Ç—ã —Å OpenAI
### –ü–µ—Ä–≤—ã–π API –∑–∞–ø—Ä–æ—Å
```
from openai import OpenAI

client = OpenAI()

response = client.responses.create(
    model="gpt-4o-mini",
    input="–ß—Ç–æ —Ç–∞–∫–æ–µ LLM?"
)

print(response.output_text)
```

### chat.completions
```
from openai import OpenAI

client = OpenAI()

response = client.chat.completions.create(
    model="gpt-4o-mini",
    messages=[
        {"role": "user", "content": "–ß—Ç–æ —Ç–∞–∫–æ–µ LLM?"}
    ],
    temperature=1.0
)

print(response.choices[0].message.content)
```

### Realtime API
```
from openai import OpenAI

client = OpenAI()

stream = client.chat.completions.create(
    model="gpt-4o-mini",
    messages=[
        {"role": "user", "content": "–ß—Ç–æ —Ç–∞–∫–æ–µ LLM?"}
    ],
    stream=True,
)

for event in stream:
    delta = event.choices[0].delta
    if delta and delta.content:
        print(delta.content, end="", flush=True)
```

### JSON response and tokens
```
from openai import OpenAI
import json

client = OpenAI()

response = client.responses.create(
    model="gpt-4o-mini",
    input="–ß—Ç–æ —Ç–∞–∫–æ–µ LLM?"
)

print(response.output_text)

response_dict = response.model_dump()
print(json.dumps(response_dict, indent=2,ensure_ascii=False))
```

### Tokens usage
```
from openai import OpenAI

client = OpenAI()

response = client.responses.create(
    model="gpt-4o-mini",
    input="–ß—Ç–æ —Ç–∞–∫–æ–µ LLM?"
)

print(response.output_text)

print(f"Input tokens: {response.usage.input_tokens}")
print(f"Output tokens: {response.usage.output_tokens}")
print(f"Total tokens: {response.usage.total_tokens}")
```

### Tokens usage: tiktoken and tokenizer
```
#pip install tiktoken

import tiktoken

enc = tiktoken.encoding_for_model("gpt-4o-mini")
text = "–ö–∞–∫ LLM —Ä–∞–∑–±–∏–≤–∞–µ—Ç —Å–ª–æ–≤–∞ –Ω–∞ —Ç–æ–∫–µ–Ω—ã?"
tokens = enc.encode(text)

for token in tokens:
    print(f"{token} ‚Üí '{enc.decode([token])}'")
```

### Structured Outputs example
```
from openai import OpenAI
from pydantic import BaseModel
from ipaddress import IPv4Address

client = OpenAI()

class Device(BaseModel):
   name: str
   ip: IPv4Address  # üëà –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ—Ç, —á—Ç–æ —Ç–æ–ª—å–∫–æ –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π IPv4 –ø–æ–ø–∞–¥—ë—Ç —Å—é–¥–∞

response = client.responses.parse(
   model="gpt-4o-mini",
   input=[
       {"role": "system", "content": "Extract device name and IP address."},
       {"role": "user", "content": "Device router01 has IP 192.168.1.254."}
   ],
   text_format=Device
)
# –≤—ã–≤–æ–¥ –∫–∞–∫ JSON
print(response.output_parsed.model_dump_json(indent=2))
```

### Conversation with LLM
```
from openai import OpenAI

client = OpenAI()

response = client.responses.create(
    model="gpt-4o-mini",
    input=[
        {"role": "user", "content": "The server is down"},
        {"role": "assistant", "content": "Did you try reboot it?"},
        {"role": "user", "content": "Of course. Still nothing"},
    ],
)
```

### Managing tokens
```
from openai import OpenAI

client = OpenAI()

response = client.responses.parse(
   model="gpt-4o-mini",
   input=[
       {"role": "system", "content": "Extract device name and IP address."},
       {"role": "user", "content": "Device router01 has IP 192.168.1.254."}
   ],
   max_output_tokens=300, # üëà –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –Ω–µ –±–æ–ª–µ–µ 300 —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è –æ—Ç–≤–µ—Ç–∞
)
```
